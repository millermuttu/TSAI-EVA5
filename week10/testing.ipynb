{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dataset\n",
    "import models\n",
    "import utils\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from train_test import train, test\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available? True\n"
     ]
    }
   ],
   "source": [
    "cuda, device = utils.others.initialize_device(utils.config.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dl = dataset.Load_dataset('cifar10', train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = dl.mean\n",
    "std = dl.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations_list = [A.Normalize(mean, std)]\n",
    "augmentations_list = [A.HorizontalFlip(), \n",
    "                      A.Rotate(limit=5),\n",
    "                     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dl = dataset.Load_dataset('cifar10', train=True, transformations=transformations_list, augmentations=augmentations_list,\\\n",
    "                         train_batch_size=32,val_batch_size=32,num_workers=4,cuda=cuda)\n",
    "\n",
    "\n",
    "# Create train data loader\n",
    "train_dl = dl.loader(train=True)\n",
    "\n",
    "# Create val data loader\n",
    "test_dl = dl.loader(train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x18ffef639b0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\sajjan\\.conda\\envs\\muttu\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 185, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"C:\\Users\\sajjan\\.conda\\envs\\muttu\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\sajjan\\.conda\\envs\\muttu\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\sajjan\\.conda\\envs\\muttu\\lib\\site-packages\\torchvision\\datasets\\cifar.py\", line 113, in __getitem__\n    img = self.transform(img)\n  File \"C:\\Users\\sajjan\\Desktop\\muttu\\TSAI-EVA5\\week10\\dataset\\data_processing.py\", line 55, in __call__\n    image = self.transform(image=image)['image']\n  File \"C:\\Users\\sajjan\\.conda\\envs\\muttu\\lib\\site-packages\\albumentations\\core\\composition.py\", line 176, in __call__\n    data = t(force_apply=force_apply, **data)\n  File \"C:\\Users\\sajjan\\.conda\\envs\\muttu\\lib\\site-packages\\albumentations\\core\\transforms_interface.py\", line 87, in __call__\n    return self.apply_with_params(params, **kwargs)\n  File \"C:\\Users\\sajjan\\.conda\\envs\\muttu\\lib\\site-packages\\albumentations\\core\\transforms_interface.py\", line 100, in apply_with_params\n    res[key] = target_function(arg, **dict(params, **target_dependencies))\n  File \"C:\\Users\\sajjan\\.conda\\envs\\muttu\\lib\\site-packages\\albumentations\\augmentations\\transforms.py\", line 1438, in apply\n    return F.normalize(image, self.mean, self.std, self.max_pixel_value)\n  File \"C:\\Users\\sajjan\\.conda\\envs\\muttu\\lib\\site-packages\\albumentations\\augmentations\\functional.py\", line 140, in normalize\n    img = img.astype(np.float32)\nAttributeError: 'Tensor' object has no attribute 'astype'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-52f6f65e28bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvisualization\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\sajjan\\Desktop\\muttu\\TSAI-EVA5\\week10\\utils\\visualization.py\u001b[0m in \u001b[0;36mshow_batch\u001b[1;34m(dataloader)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mbatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_samples\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mbs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mbatch_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mone_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[0mbatch_imgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mone_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mone_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mclass_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_to_idx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sajjan\\.conda\\envs\\muttu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sajjan\\.conda\\envs\\muttu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    987\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m                 \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 989\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    990\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sajjan\\.conda\\envs\\muttu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1012\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sajjan\\.conda\\envs\\muttu\\lib\\site-packages\\torch\\_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    393\u001b[0m             \u001b[1;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 395\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\sajjan\\.conda\\envs\\muttu\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 185, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"C:\\Users\\sajjan\\.conda\\envs\\muttu\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\sajjan\\.conda\\envs\\muttu\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\sajjan\\.conda\\envs\\muttu\\lib\\site-packages\\torchvision\\datasets\\cifar.py\", line 113, in __getitem__\n    img = self.transform(img)\n  File \"C:\\Users\\sajjan\\Desktop\\muttu\\TSAI-EVA5\\week10\\dataset\\data_processing.py\", line 55, in __call__\n    image = self.transform(image=image)['image']\n  File \"C:\\Users\\sajjan\\.conda\\envs\\muttu\\lib\\site-packages\\albumentations\\core\\composition.py\", line 176, in __call__\n    data = t(force_apply=force_apply, **data)\n  File \"C:\\Users\\sajjan\\.conda\\envs\\muttu\\lib\\site-packages\\albumentations\\core\\transforms_interface.py\", line 87, in __call__\n    return self.apply_with_params(params, **kwargs)\n  File \"C:\\Users\\sajjan\\.conda\\envs\\muttu\\lib\\site-packages\\albumentations\\core\\transforms_interface.py\", line 100, in apply_with_params\n    res[key] = target_function(arg, **dict(params, **target_dependencies))\n  File \"C:\\Users\\sajjan\\.conda\\envs\\muttu\\lib\\site-packages\\albumentations\\augmentations\\transforms.py\", line 1438, in apply\n    return F.normalize(image, self.mean, self.std, self.max_pixel_value)\n  File \"C:\\Users\\sajjan\\.conda\\envs\\muttu\\lib\\site-packages\\albumentations\\augmentations\\functional.py\", line 140, in normalize\n    img = img.astype(np.float32)\nAttributeError: 'Tensor' object has no attribute 'astype'\n"
     ]
    }
   ],
   "source": [
    "utils.visualization.show_batch(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define model\n",
    "model = models.Quiz_net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 8, 32, 32]             216\n",
      "              ReLU-2            [-1, 8, 32, 32]               0\n",
      "       BatchNorm2d-3            [-1, 8, 32, 32]              16\n",
      "           Dropout-4            [-1, 8, 32, 32]               0\n",
      "            Conv2d-5           [-1, 11, 32, 32]           1,089\n",
      "              ReLU-6           [-1, 11, 32, 32]               0\n",
      "       BatchNorm2d-7           [-1, 11, 32, 32]              22\n",
      "           Dropout-8           [-1, 11, 32, 32]               0\n",
      "         MaxPool2d-9           [-1, 22, 16, 16]               0\n",
      "           Conv2d-10            [-1, 3, 16, 16]              66\n",
      "             ReLU-11            [-1, 3, 16, 16]               0\n",
      "      BatchNorm2d-12            [-1, 3, 16, 16]               6\n",
      "          Dropout-13            [-1, 3, 16, 16]               0\n",
      "           Conv2d-14           [-1, 25, 16, 16]           5,625\n",
      "             ReLU-15           [-1, 25, 16, 16]               0\n",
      "      BatchNorm2d-16           [-1, 25, 16, 16]              50\n",
      "          Dropout-17           [-1, 25, 16, 16]               0\n",
      "           Conv2d-18           [-1, 50, 16, 16]          22,500\n",
      "             ReLU-19           [-1, 50, 16, 16]               0\n",
      "      BatchNorm2d-20           [-1, 50, 16, 16]             100\n",
      "          Dropout-21           [-1, 50, 16, 16]               0\n",
      "        MaxPool2d-22             [-1, 78, 8, 8]               0\n",
      "           Conv2d-23             [-1, 32, 8, 8]           2,496\n",
      "             ReLU-24             [-1, 32, 8, 8]               0\n",
      "      BatchNorm2d-25             [-1, 32, 8, 8]              64\n",
      "          Dropout-26             [-1, 32, 8, 8]               0\n",
      "           Conv2d-27            [-1, 110, 8, 8]         108,900\n",
      "             ReLU-28            [-1, 110, 8, 8]               0\n",
      "      BatchNorm2d-29            [-1, 110, 8, 8]             220\n",
      "          Dropout-30            [-1, 110, 8, 8]               0\n",
      "           Conv2d-31            [-1, 220, 8, 8]         435,600\n",
      "             ReLU-32            [-1, 220, 8, 8]               0\n",
      "      BatchNorm2d-33            [-1, 220, 8, 8]             440\n",
      "          Dropout-34            [-1, 220, 8, 8]               0\n",
      "        AvgPool2d-35            [-1, 220, 1, 1]               0\n",
      "           Conv2d-36             [-1, 10, 1, 1]           2,200\n",
      "================================================================\n",
      "Total params: 579,610\n",
      "Trainable params: 579,610\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.99\n",
      "Params size (MB): 2.21\n",
      "Estimated Total Size (MB): 4.22\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "utils.others.summary(model, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.config.EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                         | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Epoch 1/15\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1563 [00:15<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\sajjan\\.conda\\envs\\muttu\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 185, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"C:\\Users\\sajjan\\.conda\\envs\\muttu\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\sajjan\\.conda\\envs\\muttu\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\sajjan\\.conda\\envs\\muttu\\lib\\site-packages\\torchvision\\datasets\\cifar.py\", line 113, in __getitem__\n    img = self.transform(img)\n  File \"C:\\Users\\sajjan\\Desktop\\muttu\\TSAI-EVA5\\week10\\dataset\\data_processing.py\", line 55, in __call__\n    image = self.transform(image=image)['image']\n  File \"C:\\Users\\sajjan\\.conda\\envs\\muttu\\lib\\site-packages\\albumentations\\core\\composition.py\", line 176, in __call__\n    data = t(force_apply=force_apply, **data)\n  File \"C:\\Users\\sajjan\\.conda\\envs\\muttu\\lib\\site-packages\\albumentations\\core\\transforms_interface.py\", line 87, in __call__\n    return self.apply_with_params(params, **kwargs)\n  File \"C:\\Users\\sajjan\\.conda\\envs\\muttu\\lib\\site-packages\\albumentations\\core\\transforms_interface.py\", line 100, in apply_with_params\n    res[key] = target_function(arg, **dict(params, **target_dependencies))\n  File \"C:\\Users\\sajjan\\.conda\\envs\\muttu\\lib\\site-packages\\albumentations\\augmentations\\transforms.py\", line 1438, in apply\n    return F.normalize(image, self.mean, self.std, self.max_pixel_value)\n  File \"C:\\Users\\sajjan\\.conda\\envs\\muttu\\lib\\site-packages\\albumentations\\augmentations\\functional.py\", line 140, in normalize\n    img = img.astype(np.float32)\nAttributeError: 'Tensor' object has no attribute 'astype'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-e577c434e27c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Running Epoch {epoch+1}/{utils.config.EPOCHS}\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmisclassified_imgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sajjan\\Desktop\\muttu\\TSAI-EVA5\\week10\\train_test.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, dataloader, optimizer, loss_fn, device, train_losses, train_acc)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mprocessed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mtrain_epoch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpbar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sajjan\\.conda\\envs\\muttu\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1133\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1134\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m                 \u001b[1;31m# Update and possibly print the progressbar.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sajjan\\.conda\\envs\\muttu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sajjan\\.conda\\envs\\muttu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    987\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m                 \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 989\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    990\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sajjan\\.conda\\envs\\muttu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1012\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sajjan\\.conda\\envs\\muttu\\lib\\site-packages\\torch\\_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    393\u001b[0m             \u001b[1;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 395\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\sajjan\\.conda\\envs\\muttu\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 185, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"C:\\Users\\sajjan\\.conda\\envs\\muttu\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\sajjan\\.conda\\envs\\muttu\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\sajjan\\.conda\\envs\\muttu\\lib\\site-packages\\torchvision\\datasets\\cifar.py\", line 113, in __getitem__\n    img = self.transform(img)\n  File \"C:\\Users\\sajjan\\Desktop\\muttu\\TSAI-EVA5\\week10\\dataset\\data_processing.py\", line 55, in __call__\n    image = self.transform(image=image)['image']\n  File \"C:\\Users\\sajjan\\.conda\\envs\\muttu\\lib\\site-packages\\albumentations\\core\\composition.py\", line 176, in __call__\n    data = t(force_apply=force_apply, **data)\n  File \"C:\\Users\\sajjan\\.conda\\envs\\muttu\\lib\\site-packages\\albumentations\\core\\transforms_interface.py\", line 87, in __call__\n    return self.apply_with_params(params, **kwargs)\n  File \"C:\\Users\\sajjan\\.conda\\envs\\muttu\\lib\\site-packages\\albumentations\\core\\transforms_interface.py\", line 100, in apply_with_params\n    res[key] = target_function(arg, **dict(params, **target_dependencies))\n  File \"C:\\Users\\sajjan\\.conda\\envs\\muttu\\lib\\site-packages\\albumentations\\augmentations\\transforms.py\", line 1438, in apply\n    return F.normalize(image, self.mean, self.std, self.max_pixel_value)\n  File \"C:\\Users\\sajjan\\.conda\\envs\\muttu\\lib\\site-packages\\albumentations\\augmentations\\functional.py\", line 140, in normalize\n    img = img.astype(np.float32)\nAttributeError: 'Tensor' object has no attribute 'astype'\n"
     ]
    }
   ],
   "source": [
    "model = models.ResNet18().to(device)\n",
    "loss_fn = utils.others.cross_entropy_loss_fn()\n",
    "optimizer = utils.others.sgd_optimizer(model)\n",
    "scheduler = utils.others.StepLR_scheduler(optimizer, step_size=6)\n",
    "if utils.config.DEBUG == True:\n",
    "    utils.config.EPOCHS = 1\n",
    "\n",
    "test_loss = []\n",
    "test_accuracy = []\n",
    "train_losses = []\n",
    "train_accuracy = []\n",
    "misclassified_imgs = []\n",
    "for epoch in range(utils.config.EPOCHS):\n",
    "    print(f\"Running Epoch {epoch+1}/{utils.config.EPOCHS}\\n\")\n",
    "    train(model, train_dl, optimizer, loss_fn, device, train_losses, train_accuracy)\n",
    "    scheduler.step()\n",
    "    test(model, test_dl, loss_fn, device, 25, test_loss, test_accuracy, misclassified_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "def idx_to_class(class_to_idx):\n",
    "    return {v: k for k, v in list(class_to_idx.items())}\n",
    "\n",
    "def show_grad_cam(model,dataloader):\n",
    "    bs = dataloader.batch_size\n",
    "    num_samples = dataloader.dataset.data.shape[0]\n",
    "    batches = num_samples // bs\n",
    "    batch_id = np.random.choice(batches)\n",
    "    one_batch = list(dataloader)[batch_id]\n",
    "    batch_imgs, batch_labels = one_batch[0], one_batch[1]\n",
    "    print(batch_imgs)\n",
    "    grad_cam = utils.grad_cam.GradCam(model=model, feature_module=model.layer4, \\\n",
    "                       target_layer_names=[\"2\"], use_cuda=False)\n",
    "    class_idx = dataloader.dataset.class_to_idx\n",
    "    idx_class = idx_to_class(class_idx)\n",
    "    n_rows = n_cols = int(np.sqrt(len(batch_imgs)))\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 15))\n",
    "    if batch_imgs.shape[1] == 1:\n",
    "        cmap = 'gray'\n",
    "    else:\n",
    "        cmap = None\n",
    "    for img,label in zip(batch_imgs,batch_labels):\n",
    "        img = np.clip(img.squeeze().permute(1, 2, 0).numpy(), 0, 1)\n",
    "        img = np.float32(cv2.resize(img, (224, 224)))\n",
    "        input = utils.grad_cam.preprocess_image(img)\n",
    "\n",
    "        # If None, returns the map for the highest scoring category.\n",
    "        # Otherwise, targets the requested index.\n",
    "        target_index = None\n",
    "        mask = grad_cam(input, target_index)\n",
    "        show_cam_on_image(img, mask)\n",
    "\n",
    "        gb_model = utils.grad_cam.GuidedBackpropReLUModel(model=model, use_cuda=args.use_cuda)\n",
    "        print(model._modules.items())\n",
    "        gb = gb_model(input, index=target_index)\n",
    "        gb = gb.transpose((1, 2, 0))\n",
    "        cam_mask = cv2.merge([mask, mask, mask])\n",
    "        cam_gb = deprocess_image(cam_mask * gb)\n",
    "        gb = deprocess_image(gb)\n",
    "\n",
    "        cv2.imshow(f'gb : {idx_class[label.item()]}', gb)\n",
    "        cv2.imshow(f'cam_gb : {idx_class[label.item()]}', cam_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_grad_cam(model,test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradCam:\n",
    "    def __init__(self, model, layers):\n",
    "        self.model = model\n",
    "        self.layers = layers\n",
    "        self.hooks = []\n",
    "        self.fmap_pool = dict()\n",
    "        self.grad_pool = dict()\n",
    "\n",
    "        def forward_hook(module, input, output):\n",
    "            self.fmap_pool[module] = output.detach().cpu()\n",
    "        def backward_hook(module, grad_in, grad_out):\n",
    "            self.grad_pool[module] = grad_out[0].detach().cpu()\n",
    "        \n",
    "        for layer in layers:\n",
    "            self.hooks.append(layer.register_forward_hook(forward_hook))\n",
    "            self.hooks.append(layer.register_backward_hook(backward_hook))\n",
    "\n",
    "    def close(self):\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        self.close()\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        self.model.zero_grad()\n",
    "        return self.model(*args, **kwargs)\n",
    "\n",
    "    def get(self, layer):\n",
    "        assert layer in self.layers, f'{layer} not in {self.layers}'\n",
    "        fmap_b = self.fmap_pool[layer] # [N, C, fmpH, fmpW]\n",
    "        grad_b = self.grad_pool[layer] # [N, C, fmpH, fmpW]\n",
    "\n",
    "        grad_b = F.adaptive_avg_pool2d(grad_b, (1, 1)) # [N, C, 1, 1]\n",
    "        gcam_b = (fmap_b * grad_b).sum(dim=1, keepdim=True) # [N, 1, fmpH, fmpW]\n",
    "        gcam_b = F.relu(gcam_b)\n",
    "\n",
    "        return gcam_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GuidedBackPropogation:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.hooks = []\n",
    "\n",
    "        def backward_hook(module, grad_in, grad_out):\n",
    "            if isinstance(module, nn.ReLU):\n",
    "                return tuple(grad.clamp(min=0.0) for grad in grad_in)\n",
    "\n",
    "        for name, module in self.model.named_modules():\n",
    "            self.hooks.append(module.register_backward_hook(backward_hook))\n",
    "\n",
    "    def close(self):\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        self.close()\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        self.model.zero_grad()\n",
    "        return self.model(*args, **kwargs)\n",
    "    \n",
    "    def get(self, layer):\n",
    "        return layer.grad.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorize(tensor, colormap=plt.cm.jet):\n",
    "    '''Apply colormap to tensor\n",
    "    Args:\n",
    "        tensor: (FloatTensor), sized [N, 1, H, W]\n",
    "        colormap: (plt.cm.*)\n",
    "    Return:\n",
    "        tensor: (FloatTensor), sized [N, 3, H, W]\n",
    "    '''\n",
    "    tensor = tensor.clamp(min=0.0)\n",
    "    tensor = tensor.squeeze(dim=1).numpy() # [N, H, W]\n",
    "    tensor = colormap(tensor)[..., :3] # [N, H, W, 3]\n",
    "    tensor = torch.from_numpy(tensor).float()\n",
    "    tensor = tensor.permute(0, 3, 1, 2) # [N, 3, H, W]\n",
    "    return tensor\n",
    "\n",
    "def normalize(tensor, eps=1e-8):\n",
    "    '''Normalize each tensor in mini-batch like Min-Max Scaler\n",
    "    Args:\n",
    "        tensor: (FloatTensor), sized [N, C, H, W]\n",
    "    Return:\n",
    "        tensor: (FloatTensor) ranged [0, 1], sized [N, C, H, W]\n",
    "    '''\n",
    "    N = tensor.size(0)\n",
    "    min_val = tensor.contiguous().view(N, -1).min(dim=1)[0]\n",
    "    tensor = tensor - min_val.view(N, 1, 1, 1)\n",
    "    max_val = tensor.contiguous().view(N, -1).max(dim=1)[0]\n",
    "    tensor = tensor / (max_val + eps).view(N, 1, 1, 1)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_gad_cam(model,device,dataloader):\n",
    "    model.eval()\n",
    "    dataiter = iter(dataloader)\n",
    "    images, labels = dataiter.next()\n",
    "    for img, lbl in zip(images,labels):\n",
    "        inp_b = img.unsqueeze(dim=0) # [N, 3, 224, 224]\n",
    "        inp_b = inp_b.to(device)\n",
    "        \n",
    "        with GradCam(model, [model.layer4]) as gcam:\n",
    "            out_b = gcam(inp_b) # [N, C]\n",
    "            out_b[:, labels[0]].backward()\n",
    "\n",
    "            gcam_b = gcam.get(model.layer4) # [N, 1, fmpH, fmpW]\n",
    "            gcam_b = F.interpolate(gcam_b, [32, 32], mode='bilinear', align_corners=False) # [N, 1, inpH, inpW]\n",
    "            save_image(normalize(gcam_b), './gcam.png')\n",
    "\n",
    "\n",
    "        with GuidedBackPropogation(model) as gdbp:\n",
    "            inp_b = inp_b.requires_grad_() # Enable recording inp_b's gradient\n",
    "            out_b = gdbp(inp_b)\n",
    "            out_b[:, labels[0]].backward()\n",
    "\n",
    "            grad_b = gdbp.get(inp_b) # [N, 3, inpH, inpW]\n",
    "            grad_b = grad_b.mean(dim=1, keepdim=True) # [N, 1, inpH, inpW]\n",
    "            save_image(normalize(grad_b), './grad.png')\n",
    "            \n",
    "        mixed = gcam_b * grad_b\n",
    "        heatmap = normalize(mixed)\n",
    "        img = img / 2 + 0.49139968     # unnormalize\n",
    "        npimg = img.numpy()\n",
    "        \n",
    "        ## plotting the image and gradmap\n",
    "        f, axarr = plt.subplots(nrows=1,ncols=2)\n",
    "        plt.sca(axarr[0]); \n",
    "        plt.imshow(heatmap.squeeze()); plt.title('image')\n",
    "        plt.sca(axarr[1]);\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "        # plt.imshow(img); \n",
    "        plt.title('grad_cam')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_gad_cam(model,device,test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.models import densenet121\n",
    "from torchvision.transforms import functional as tf\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "seed = 999\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device('cuda')\n",
    "model = model\n",
    "model.eval()\n",
    "\n",
    "dataiter = iter(test_dl)\n",
    "images, labels = dataiter.next()\n",
    "img = images[0]\n",
    "\n",
    "# img = Image.open('./samples/cat_dog.png')\n",
    "# img = img.resize((224, 224))\n",
    "# img = tf.to_tensor(img) # [3, 224, 224]\n",
    "# img = tf.normalize(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "inp_b = img.unsqueeze(dim=0) # [N, 3, 224, 224]\n",
    "inp_b = inp_b.to(device)\n",
    "\n",
    "# 243: boxer\n",
    "# 283: tiger cat\n",
    "# grad_b = torch.zeros_like(out_b, device=device)\n",
    "# grad_b[:, out_b.argmax(dim=1)] = +1.0\n",
    "# out_b.backward(gradient=grad_b)\n",
    "\n",
    "with GradCam(model, [model.layer4]) as gcam:\n",
    "    out_b = gcam(inp_b) # [N, C]\n",
    "    out_b[:, labels[0]].backward()\n",
    "\n",
    "    gcam_b = gcam.get(model.layer4) # [N, 1, fmpH, fmpW]\n",
    "    gcam_b = F.interpolate(gcam_b, [32, 32], mode='bilinear', align_corners=False) # [N, 1, inpH, inpW]\n",
    "    save_image(normalize(gcam_b), './gcam.png')\n",
    "\n",
    "\n",
    "with GuidedBackPropogation(model) as gdbp:\n",
    "    inp_b = inp_b.requires_grad_() # Enable recording inp_b's gradient\n",
    "    out_b = gdbp(inp_b)\n",
    "    out_b[:, labels[0]].backward()\n",
    "\n",
    "    grad_b = gdbp.get(inp_b) # [N, 3, inpH, inpW]\n",
    "    grad_b = grad_b.mean(dim=1, keepdim=True) # [N, 1, inpH, inpW]\n",
    "    save_image(normalize(grad_b), './grad.png')\n",
    "\n",
    "\n",
    "mixed = gcam_b * grad_b\n",
    "heatmap = normalize(mixed)\n",
    "img = img / 2 + 0.49139968     # unnormalize\n",
    "npimg = img.numpy()\n",
    "# colorized = colorize(mixed)\n",
    "# save_image(mixed, './mixed.png')\n",
    "# plt.matshow(heatmap.squeeze())\n",
    "\n",
    "# plt.show()\n",
    "# # heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "# heatmap = np.uint8(255 * heatmap)\n",
    "# np.uint8(img)\n",
    "# superimposed_img = heatmap * 0.4 + img.numpy()\n",
    "# trans = transforms.ToPILImage()\n",
    "# plt.imshow(trans(superimposed_img[0]))\n",
    "# # cv2.imwrite('./map.jpg', superimposed_img)\n",
    "\n",
    "f, axarr = plt.subplots(nrows=1,ncols=2)\n",
    "plt.sca(axarr[0]); \n",
    "plt.imshow(heatmap.squeeze()); plt.title('image')\n",
    "plt.sca(axarr[1]);\n",
    "plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "# plt.imshow(img); \n",
    "plt.title('grad_cam')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(superimposed_img), superimposed_img.shape,superimposed_img.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.ToPILImage()\n",
    "heatmap = np.maximum(mixed, 0)\n",
    "heatmap /= torch.max(heatmap)\n",
    "plt.matshow(heatmap.squeeze())\n",
    "# gradmap = trans(mixed[0])\n",
    "# plt.imshow(trans(colorized[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(trans(colorized[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deprocess_image(img):\n",
    "    \"\"\" see https://github.com/jacobgil/keras-grad-cam/blob/master/grad-cam.py#L65 \"\"\"\n",
    "    img = img - np.mean(img)\n",
    "    img = img / (np.std(img) + 1e-5)\n",
    "    img = img * 0.1\n",
    "    img = img + 0.5\n",
    "    img = np.clip(img, 0, 1)\n",
    "    return np.uint8(img*255)\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.49139968     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\tdef overlay_heatmap(heatmap, image, alpha=0.5,\n",
    "\t\tcolormap=cv2.COLORMAP_VIRIDIS):\n",
    "\t\t# apply the supplied color map to the heatmap and then\n",
    "\t\t# overlay the heatmap on the input image\n",
    "\t\theatmap = cv2.applyColorMap(heatmap, colormap)\n",
    "\t\toutput = cv2.addWeighted(image, alpha, heatmap, 1 - alpha, 0)\n",
    "\t\t# return a 2-tuple of the color mapped heatmap and the output,\n",
    "\t\t# overlaid image\n",
    "\t\treturn (heatmap, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap, output = overlay_heatmap(gradmap,img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax1.imshow(img)\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "ax2.imshow(gradmap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = gradmap*mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
